{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Activation and Gradients Compression for Model Parallelism\n\nExperiment code for resnet&cifar training with compression.","metadata":{"pycharm":{"name":"#%% md\n"},"id":"jZ82dKG2LWwC"}},{"cell_type":"markdown","source":"## Imports, Installs, Versions","metadata":{"pycharm":{"name":"#%% md\n"},"id":"wOuAPVyZLWwD"}},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2023-05-22T20:17:13.403945Z","iopub.execute_input":"2023-05-22T20:17:13.404767Z","iopub.status.idle":"2023-05-22T20:17:28.575399Z","shell.execute_reply.started":"2023-05-22T20:17:13.404716Z","shell.execute_reply":"2023-05-22T20:17:28.574174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport sys\nimport random\nimport copy\nimport wandb\n# import lovely_tensors as lt\n# lt.monkey_patch()\nprint(\"Python Version: \", sys.version)\nprint(\"PyTorch Version: \", torch.__version__)\nprint(\"Torchvision Version: \", torchvision.__version__)\nprint(\"WandB version: \", wandb.__version__)\n\nseed = 801\ntorch.manual_seed(seed)\nrandom.seed(seed)\nnp.random.seed(seed)","metadata":{"pycharm":{"name":"#%%\n"},"id":"jT15qEg1LWwE","outputId":"2fac166d-3e23-4cec-89a5-85cd24501685","execution":{"iopub.status.busy":"2023-05-22T20:19:54.259605Z","iopub.execute_input":"2023-05-22T20:19:54.260969Z","iopub.status.idle":"2023-05-22T20:19:57.663341Z","shell.execute_reply.started":"2023-05-22T20:19:54.260913Z","shell.execute_reply":"2023-05-22T20:19:57.662071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Config","metadata":{"pycharm":{"name":"#%% md\n"},"id":"rn3s9NN_LWwF"}},{"cell_type":"code","source":"if 'KAGGLE_URL_BASE' in os.environ:\n    kaggle_run = True\nelse:\n    kaggle_run = False\nprint(f'Run on kaggle: {kaggle_run}')","metadata":{"pycharm":{"name":"#%%\n"},"id":"FS8UdojpLWwF","outputId":"a643098c-009e-44f3-e64a-c585f1165c36","execution":{"iopub.status.busy":"2023-05-22T20:19:57.666004Z","iopub.execute_input":"2023-05-22T20:19:57.666377Z","iopub.status.idle":"2023-05-22T20:19:57.672735Z","shell.execute_reply.started":"2023-05-22T20:19:57.666334Z","shell.execute_reply":"2023-05-22T20:19:57.671666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if kaggle_run:\n  from kaggle_secrets import UserSecretsClient\n  user_secrets = UserSecretsClient()\n  secret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")\n  os.environ[\"WANDB_API_KEY\"] = secret_value_0\n# wandb.login()\nuse_wandb = True","metadata":{"pycharm":{"name":"#%%\n"},"id":"3qzvdeYSLWwG","execution":{"iopub.status.busy":"2023-05-22T20:19:57.674276Z","iopub.execute_input":"2023-05-22T20:19:57.674919Z","iopub.status.idle":"2023-05-22T20:19:58.049656Z","shell.execute_reply.started":"2023-05-22T20:19:57.674881Z","shell.execute_reply":"2023-05-22T20:19:58.048527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compression_config = {\n#     'layer1':{\n#         'forward': 'id',\n#         'forward-EF': False,\n#         'forward-params': {},\n#         'backward': 'id',\n#         'backward-EF': False,\n#         'backward-params': {},\n#     },\n#     'layer2':{\n#         'forward': 'id',\n#         'forward-EF': False,\n#         'forward-params': {},\n#         'backward': 'id',\n#         'backward-EF': False,\n#         'backward-params': {},\n#     },\n#     'layer3':{\n#         'forward': 'id',\n#         'forward-EF': False,\n#         'forward-params': {},\n#         'backward': 'id',\n#         'backward-EF': False,\n#         'backward-params': {},\n#     },\n# }","metadata":{"execution":{"iopub.status.busy":"2023-05-22T20:19:58.052215Z","iopub.execute_input":"2023-05-22T20:19:58.052687Z","iopub.status.idle":"2023-05-22T20:19:58.058165Z","shell.execute_reply.started":"2023-05-22T20:19:58.052646Z","shell.execute_reply":"2023-05-22T20:19:58.056995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nDict-based config for compression\n\"\"\"\ncompression_config = {\n    'layer1':{\n        'forward': 'topk',\n        'forward-EF': False,\n        'forward-params': {'topk': 0.05},\n        'backward': 'topk',\n        'backward-EF': False,\n        'backward-params': {'topk': 0.1},\n    },\n    'layer2':{\n        'forward': 'topk',\n        'forward-EF': False,\n        'forward-params': {'topk': 0.05},\n        'backward': 'topk',\n        'backward-EF': False,\n        'backward-params': {'topk': 0.1},\n    },\n    'layer3':{\n        'forward': 'topk',\n        'forward-EF': False,\n        'forward-params': {'topk': 0.05},\n        'backward': 'topk',\n        'backward-EF': False,\n        'backward-params': {'topk': 0.1},\n    },\n}","metadata":{"pycharm":{"name":"#%%\n"},"id":"bZE-8NOTLWwG","execution":{"iopub.status.busy":"2023-05-22T20:19:58.060194Z","iopub.execute_input":"2023-05-22T20:19:58.061008Z","iopub.status.idle":"2023-05-22T20:19:58.070410Z","shell.execute_reply.started":"2023-05-22T20:19:58.060953Z","shell.execute_reply":"2023-05-22T20:19:58.069012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n  \"learning_rate\": 0.01,\n  \"architecture\": \"resnet-18\",\n  \"dataset\": \"CIFAR10\",\n  \"epochs\": 100,\n  \"compression-config\": compression_config,\n  \"AC-SGD\": False,\n  \"seed\": seed,\n}","metadata":{"pycharm":{"name":"#%%\n"},"id":"xetmnwNbLWwG","execution":{"iopub.status.busy":"2023-05-22T20:19:59.395538Z","iopub.execute_input":"2023-05-22T20:19:59.396263Z","iopub.status.idle":"2023-05-22T20:19:59.401593Z","shell.execute_reply.started":"2023-05-22T20:19:59.396223Z","shell.execute_reply":"2023-05-22T20:19:59.400498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run = wandb.init(\n    project=\"resnet-cifar-compression\",\n    name='fw-top5-bw-top10',\n    mode='online' if use_wandb else 'disabled',\n    config=config,\n)","metadata":{"pycharm":{"name":"#%%\n"},"id":"z4vQ8kfkLWwH","execution":{"iopub.status.busy":"2023-05-22T20:20:01.217524Z","iopub.execute_input":"2023-05-22T20:20:01.218447Z","iopub.status.idle":"2023-05-22T20:20:01.965075Z","shell.execute_reply.started":"2023-05-22T20:20:01.218402Z","shell.execute_reply":"2023-05-22T20:20:01.964033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training implementation is based on:\nhttps://github.com/kuangliu/pytorch-cifar","metadata":{"pycharm":{"name":"#%% md\n"},"id":"sGb_17Z7LWwH"}},{"cell_type":"markdown","source":"## Dataset","metadata":{"pycharm":{"name":"#%% md\n"},"id":"fsCJJmNvLWwH"}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass CIFAR10(Dataset):\n    def __init__(self, train=True, with_idx=False):\n\n        transform_train = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n        ])\n\n        transform_test = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n        ])\n\n        transform = transform_train if train else transform_test\n        self.with_idx = with_idx\n        self.cifar10 = torchvision.datasets.CIFAR10(\n            root='./data',\n            train=train,\n            download=True,\n            transform=transform)\n\n    def __getitem__(self, index):\n        data, target = self.cifar10[index]\n        if self.with_idx:\n            return data, target, index\n        else:\n            return data, target\n\n    def __len__(self):\n        return len(self.cifar10)","metadata":{"pycharm":{"name":"#%%\n"},"id":"HzvO6L9qLWwH","execution":{"iopub.status.busy":"2023-05-22T20:20:04.204263Z","iopub.execute_input":"2023-05-22T20:20:04.205237Z","iopub.status.idle":"2023-05-22T20:20:04.217151Z","shell.execute_reply.started":"2023-05-22T20:20:04.205182Z","shell.execute_reply":"2023-05-22T20:20:04.215799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with_idx = config[\"AC-SGD\"]\nbatch_size = 128\n\ntrainloader = torch.utils.data.DataLoader(\n    CIFAR10(with_idx=with_idx),\n    batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True\n)\n\ntestloader = torch.utils.data.DataLoader(\n    CIFAR10(train=False, with_idx=with_idx),\n    batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True\n)\n\n# classes = ('plane', 'car', 'bird', 'cat',\n#            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')","metadata":{"pycharm":{"name":"#%%\n"},"id":"Np7E4uYVLWwI","outputId":"f21b51d9-9678-4fe4-a1fb-3b3b748b620f","execution":{"iopub.status.busy":"2023-05-22T20:20:04.346226Z","iopub.execute_input":"2023-05-22T20:20:04.346552Z","iopub.status.idle":"2023-05-22T20:20:12.626449Z","shell.execute_reply.started":"2023-05-22T20:20:04.346521Z","shell.execute_reply":"2023-05-22T20:20:12.625248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(torch.cuda.get_device_name(device))","metadata":{"execution":{"iopub.status.busy":"2023-05-22T20:20:12.632609Z","iopub.execute_input":"2023-05-22T20:20:12.635321Z","iopub.status.idle":"2023-05-22T20:20:12.720640Z","shell.execute_reply.started":"2023-05-22T20:20:12.635277Z","shell.execute_reply":"2023-05-22T20:20:12.719332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compression Functions","metadata":{}},{"cell_type":"markdown","source":"### TopK","metadata":{}},{"cell_type":"code","source":"# Function to compress input vector\n# Returns vector of the same shape, zero-out lowest(absolute value) (1-k)*100% values\ndef compress_topk(input, topk=0.1):\n    input_2d = input.flatten(start_dim=1)\n    n_lowest = int(input_2d.shape[1] * (1 - topk))\n    input_2d_abs = torch.abs(input_2d)\n    pivot = torch.kthvalue(input_2d_abs,k=n_lowest, keepdim=True).values\n    mask = input_2d_abs <= pivot\n    out = input_2d.masked_fill_(mask, 0)\n    return out.reshape(input.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T20:21:56.201263Z","iopub.execute_input":"2023-05-22T20:21:56.201659Z","iopub.status.idle":"2023-05-22T20:21:56.210108Z","shell.execute_reply.started":"2023-05-22T20:21:56.201623Z","shell.execute_reply":"2023-05-22T20:21:56.208788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom Quantization","metadata":{}},{"cell_type":"code","source":"def quantize_custom(input, k=2):\n    x = input.flatten(start_dim=1)\n    x_min = x.min()\n    x_max = x.max()\n    x_norm = (x - x_min) / (x_max - x_min) # from 0 to 1\n    val = 1/(2**k-1) * torch.round((2**k - 1) * x_norm) # quantized\n    out = val * (x_max - x_min) + x_min # back to scale\n    return out.reshape(input.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T20:21:57.443929Z","iopub.execute_input":"2023-05-22T20:21:57.444663Z","iopub.status.idle":"2023-05-22T20:21:57.451629Z","shell.execute_reply.started":"2023-05-22T20:21:57.444625Z","shell.execute_reply":"2023-05-22T20:21:57.450476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AC-SGD Quantization\n\nreference:https://github.com/DS3Lab/AC-SGD/blob/6c1d90093f700ed6716dda3e77733160f3794148/compress/fixpoint.py#L22","metadata":{}},{"cell_type":"code","source":"def quantize_ACSGD(x, nbits, scale_method='max', scale_dims=(0,)):\n    \n    fbits = nbits - 1\n    \n    if scale_method == 'max':\n        # issue: sensitive to outlier points\n        scale = x.abs().amax(scale_dims, keepdims=True)\n    elif scale_method == 'l2':\n        # ~95% confidence interval for normal distribution\n        scale = x.pow(2).mean(scale_dims, keepdims=True).sqrt() * 2 \n    else:\n        raise Exception('unkonwn scale method.')\n    # fp16 should be enough\n    scale = scale.half()\n    x = x / (scale + 1e-6)\n    \n    x = x.ldexp(torch.tensor(fbits))\n    clip_min = -(1<<fbits)\n    clip_max = (1<<fbits)-1\n\n    x = x.round()\n    x = x.clip(clip_min, clip_max)\n    \n    x = x - clip_min\n    x = x.type(torch.uint8)\n    \n    return x, scale\n\ndef dequantize_ACSGD(x, nbits, scale):\n    \n    fbits = nbits - 1\n    \n    clip_min = -(1<<fbits)\n    clip_max = (1<<fbits)-1\n    \n    x = x.float() + clip_min\n    \n    x = x / (clip_max+1) * scale\n    \n    return x\n\n# XXX: works only for vectors on GPU\ndef quantize_dequantize_ACSGD(x, nbits, scale_method='max', scale_dims=(0,)):\n    xx = x.flatten(start_dim=1)\n    xx, scale = quantize_ACSGD(xx, nbits, scale_method, scale_dims)\n    c_x = dequantize_ACSGD(xx, nbits, scale)\n    return c_x","metadata":{"execution":{"iopub.status.busy":"2023-05-22T20:21:58.400488Z","iopub.execute_input":"2023-05-22T20:21:58.400910Z","iopub.status.idle":"2023-05-22T20:21:58.415347Z","shell.execute_reply.started":"2023-05-22T20:21:58.400872Z","shell.execute_reply":"2023-05-22T20:21:58.413993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Torch Autograd Compressors","metadata":{}},{"cell_type":"code","source":"def BaseCompressor(compressor_forward, compressor_backward):\n\n    class myCompressor(torch.autograd.Function):\n\n            @staticmethod\n            def forward(ctx, input, compress=True):\n                if compressor_forward is None:\n                    return input\n                x = input.detach().clone()\n                ctx.compress = compress\n                if compress:\n                    c_x = compressor_forward(x)\n                else:\n                    c_x = x\n                return c_x\n\n            @staticmethod\n            def backward(ctx, grad_output):\n                if compressor_backward is None:\n                    return grad_output\n                x = grad_output.detach().clone()\n                if ctx.compress:\n                    c_x = compressor_backward(x)\n                else:\n                    c_x = x\n                return c_x, None\n\n    return myCompressor.apply","metadata":{"execution":{"iopub.status.busy":"2023-05-22T20:21:59.512761Z","iopub.execute_input":"2023-05-22T20:21:59.513170Z","iopub.status.idle":"2023-05-22T20:21:59.523466Z","shell.execute_reply.started":"2023-05-22T20:21:59.513133Z","shell.execute_reply":"2023-05-22T20:21:59.522007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def EFCompressor(compressor_forward, fwd_error_buffer, compressor_backward, bckwd_error_buffer):\n    \n    class EFCompression(torch.autograd.Function):\n\n            @staticmethod\n            def forward(ctx, input, compress):\n                if compressor_forward is None:\n                    return input\n                x = input.detach().clone()\n                ctx.compress = compress\n                if compress:\n#                     e = fwd_error_buffer # get error from dict\n                    c_x = compressor_forward(x) # compress x+e\n#                     e = e + x - c_x # update e\n#                     fwd_error_buffer[:] = e # set e back to dict\n                    return c_x\n                else:\n                    c_x = x\n                return c_x\n\n\n            @staticmethod\n            def backward(ctx, grad_output):\n                if compressor_backward is None:\n                    return grad_output\n                x = grad_output.detach().clone()\n                if ctx.compress:\n                    e = bckwd_error_buffer # get error from dict\n                    c_x = compressor_backward(x + e) # compress x+e\n                    e = e + x - c_x # update e\n                    bckwd_error_buffer[:] = e # set e back to dict\n                else:\n                    c_x = x\n                return c_x, None\n\n    return EFCompression.apply","metadata":{"execution":{"iopub.status.busy":"2023-05-22T20:22:00.403469Z","iopub.execute_input":"2023-05-22T20:22:00.404439Z","iopub.status.idle":"2023-05-22T20:22:00.416273Z","shell.execute_reply.started":"2023-05-22T20:22:00.404385Z","shell.execute_reply":"2023-05-22T20:22:00.414741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def EF21Compressor(compressor_forward, fwd_error_buffer, compressor_backward, bckwd_error_buffer):\n    \n    class EF21Compression(torch.autograd.Function):\n\n            @staticmethod\n            def forward(ctx, input, compress):\n                if compressor_forward is None:\n                    return input\n                x = input.detach().clone()\n                ctx.compress = compress\n                if compress:\n                    A0 = fwd_error_buffer # get old activations from dict\n                    c_x = compress_forward(x - A0) # compress P1 - A0\n                    A1 = A0 + c_x # update Ai\n                    fwd_error_buffer[:] = A1 # store A1 back to dict\n                    return A1\n                else:\n                    c_x = x\n                return c_x\n\n\n            @staticmethod\n            def backward(ctx, grad_output):\n                if compressor_backward is None:\n                    return grad_output\n                x = grad_output.detach().clone()\n                if ctx.compress:\n                    A0 = bckwd_error_buffer # get old activations from dict\n                    c_x, _ = BaseCompressor(x - A0) # compress diff of activations\n                    A1 = A0 + c_x # send old + difference compressed\n                    bckwd_error_buffer[:] = A1 # store A1 back to dict\n                    return A1, None\n                else:\n                    c_x = x\n                return c_x, None\n\n    return EF21Compression.apply","metadata":{"execution":{"iopub.status.busy":"2023-05-22T20:22:05.233048Z","iopub.execute_input":"2023-05-22T20:22:05.233514Z","iopub.status.idle":"2023-05-22T20:22:05.247065Z","shell.execute_reply.started":"2023-05-22T20:22:05.233473Z","shell.execute_reply":"2023-05-22T20:22:05.245899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compressor Module","metadata":{}},{"cell_type":"code","source":"class Compressor(nn.Module):\n    \n    compressors_dict = {\n        'id': None,\n        'topk': compress_topk,\n        'quantization_simple': quantize_custom,\n        'quantization_acsgd': quantize_dequantize_ACSGD\n    }\n    \n    def _get_compressor(self, compressor_id, **compressor_params):\n        if compressor_id in self.compressors_dict:\n            if compressor_id == 'id':\n                compressor = lambda x: x\n            else:\n                compressor = lambda x: self.compressors_dict[compressor_id](x, **compressor_params)\n        else:\n            raise ValueError(f'Compressor with name {compressor_id} Not found')\n        return compressor\n    \n    def __init__(self, input_shape, forward='id', forward_params={}, backward='id', backward_params={}, EF=False):\n        super(Compressor, self).__init__()\n        self.input_shape = input_shape\n        self.forward_func = forward\n        self.forward_params = forward_params\n        self.backward_func = backward\n        self.backward_params = backward_params\n        forward_compression = self._get_compressor(forward, **forward_params)\n        backward_compression = self._get_compressor(backward, **backward_params)\n        if EF:\n            self.EF_forward_buffer = torch.zeros(input_shape, device=device)\n            self.EF_backward_buffer = torch.zeros(input_shape, device=device)\n            self.compressor = EFCompressor(\n                forward_compression, self.EF_forward_buffer,\n                backward_compression, self.EF_backward_buffer\n            )\n        else:\n            self.compressor = BaseCompressor(\n                forward_compression, backward_compression\n            )\n        \n\n    def forward(self, x, compress=True):\n        out = self.compressor(x, compress)\n        return out\n    \n#     def __repr__(self):\n#         return('forward={} with params {} backward={} with params {}'.format(\n#             self.forward_func,\n#             (*[str(x)+'='+str(y) for x, y in self.forward_params.items()],) if self.forward_params else \"\",\n#             self.backward_func,\n#             (*[str(x)+'='+str(y) for x, y in self.backward_params.items()],) if self.backward_params else \"\",\n#         ))","metadata":{"execution":{"iopub.status.busy":"2023-05-22T20:22:06.235185Z","iopub.execute_input":"2023-05-22T20:22:06.236226Z","iopub.status.idle":"2023-05-22T20:22:06.250158Z","shell.execute_reply.started":"2023-05-22T20:22:06.236188Z","shell.execute_reply":"2023-05-22T20:22:06.248999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"pycharm":{"name":"#%% md\n"},"id":"wgF-IkK6LWwI"}},{"cell_type":"markdown","source":"ResNet-18 adopted from [https://github.com/kuangliu/pytorch-cifar](pytorch-cifar)","metadata":{"pycharm":{"name":"#%% md\n"},"id":"2BSr1GE9LWwI"}},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion *\n                               planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2, 2, 2, 2])","metadata":{"pycharm":{"name":"#%%\n"},"id":"g3oyptLtLWwI","execution":{"iopub.status.busy":"2023-05-22T20:22:17.039462Z","iopub.execute_input":"2023-05-22T20:22:17.039892Z","iopub.status.idle":"2023-05-22T20:22:17.069398Z","shell.execute_reply.started":"2023-05-22T20:22:17.039856Z","shell.execute_reply":"2023-05-22T20:22:17.068385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model With Compression","metadata":{}},{"cell_type":"code","source":"class ResNetWithCompression(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10, compression_config=None):\n        super(ResNetWithCompression, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n        \n        shapes = [(batch_size, 64, 32, 32), (batch_size, 128, 16, 16), (batch_size, 256, 8, 8)]\n        \n        for (module_name, compression_params), input_shape in zip(compression_config.items(), shapes):\n            compressor = Compressor(\n                input_shape,\n                compression_params['forward'], compression_params['forward-params'],\n                compression_params['backward'], compression_params['backward-params'],\n            )\n            setattr(self, module_name+'_compression', compressor)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x, compress=True):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1_compression(self.layer1(out), compress=compress)\n        out = self.layer2_compression(self.layer2(out), compress=compress)\n        out = self.layer3_compression(self.layer3(out), compress=compress)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNet18WithCompression(compression_config):\n    return ResNetWithCompression(BasicBlock, [2, 2, 2, 2], compression_config=compression_config)","metadata":{"pycharm":{"name":"#%%\n"},"id":"YEZN8rwhLWwJ","execution":{"iopub.status.busy":"2023-05-22T20:22:18.194172Z","iopub.execute_input":"2023-05-22T20:22:18.195155Z","iopub.status.idle":"2023-05-22T20:22:18.213615Z","shell.execute_reply.started":"2023-05-22T20:22:18.195102Z","shell.execute_reply":"2023-05-22T20:22:18.212246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Parameters","metadata":{"pycharm":{"name":"#%% md\n"},"id":"dUpHDpZoLWwM"}},{"cell_type":"code","source":"net = ResNet18WithCompression(compression_config)\nnet.to(device)","metadata":{"pycharm":{"name":"#%%\n"},"id":"61aXmhB5LWwN","outputId":"d1eac71a-07b0-4291-f1c5-ed9e660eb98b","execution":{"iopub.status.busy":"2023-05-22T20:22:31.529919Z","iopub.execute_input":"2023-05-22T20:22:31.530325Z","iopub.status.idle":"2023-05-22T20:22:34.325620Z","shell.execute_reply.started":"2023-05-22T20:22:31.530291Z","shell.execute_reply":"2023-05-22T20:22:34.324609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=config['learning_rate'],\n                      momentum=0.9, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)","metadata":{"pycharm":{"name":"#%%\n"},"id":"56hb_bfzLWwN","execution":{"iopub.status.busy":"2023-05-22T20:23:24.725440Z","iopub.execute_input":"2023-05-22T20:23:24.726257Z","iopub.status.idle":"2023-05-22T20:23:24.733933Z","shell.execute_reply.started":"2023-05-22T20:23:24.726214Z","shell.execute_reply":"2023-05-22T20:23:24.732836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_acc = 0\nbest_acc_compressed = 0","metadata":{"pycharm":{"name":"#%%\n"},"id":"ucoqtE_3LWwN","execution":{"iopub.status.busy":"2023-05-22T20:23:25.738818Z","iopub.execute_input":"2023-05-22T20:23:25.739530Z","iopub.status.idle":"2023-05-22T20:23:25.744345Z","shell.execute_reply.started":"2023-05-22T20:23:25.739491Z","shell.execute_reply":"2023-05-22T20:23:25.743032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train & Test Loop","metadata":{"pycharm":{"name":"#%% md\n"},"id":"VJzxYmcfLWwN"}},{"cell_type":"code","source":"def train(epoch):\n    print('\\nEpoch: %d' % epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs, compress=True)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n        if batch_idx % 100 == 99:\n            print('Train: Loss: %.3f | Acc: %.3f%% (%d/%d)'  % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n    acc = 100.*correct/total\n    wandb.log({\n        'train_loss': train_loss,\n        'train_acc': acc\n    }, commit=False)","metadata":{"pycharm":{"name":"#%%\n"},"id":"an-2sogFLWwO","execution":{"iopub.status.busy":"2023-05-22T20:25:15.320963Z","iopub.execute_input":"2023-05-22T20:25:15.321664Z","iopub.status.idle":"2023-05-22T20:25:15.332529Z","shell.execute_reply.started":"2023-05-22T20:25:15.321627Z","shell.execute_reply":"2023-05-22T20:25:15.331287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(epoch):\n    global best_acc, best_acc_compressed\n    net.eval()\n    test_loss = 0\n    test_loss_compressed = 0\n    correct = 0\n    correct_compressed = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # no compression test\n            outputs = net(inputs, compress=False)\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(targets).sum().item()\n            \n            # compressed test\n            outputs = net(inputs, compress=True)\n            loss = criterion(outputs, targets)\n            test_loss_compressed += loss.item()\n            _, predicted = outputs.max(1)\n            correct_compressed += predicted.eq(targets).sum().item()\n            \n            \n            \n            total += targets.size(0)\n    acc = 100.*correct/total\n    acc_compressed = 100.*correct_compressed/total\n    print('Test Uncompressed: Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss/len(testloader), acc, correct, total))\n    print('Test Compressed: Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss_compressed/len(testloader), acc_compressed, correct_compressed, total))\n\n    wandb.log({\n        'test_loss': test_loss,\n        'test_acc': acc,\n        'test_loss_compressed': test_loss_compressed,\n        'test_acc_compressed': acc_compressed,\n        'epoch': epoch\n    })\n    if acc > best_acc:\n        best_acc = acc\n        print(f'New best acc {acc}')\n    if acc_compressed > best_acc_compressed:\n        best_acc_compressed = acc_compressed\n        print(f'New best acc_compressed {acc_compressed}')","metadata":{"pycharm":{"name":"#%%\n"},"id":"RNvy_tBULWwO","execution":{"iopub.status.busy":"2023-05-22T20:25:16.122090Z","iopub.execute_input":"2023-05-22T20:25:16.122496Z","iopub.status.idle":"2023-05-22T20:25:16.135759Z","shell.execute_reply.started":"2023-05-22T20:25:16.122460Z","shell.execute_reply":"2023-05-22T20:25:16.134651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Model!","metadata":{"pycharm":{"name":"#%% md\n"},"id":"63d9B0M-LWwO"}},{"cell_type":"code","source":"import time\nfor epoch in range(config['epochs']):\n    s = time.time()\n    train(epoch)\n    test(epoch)\n    scheduler.step()\n    e = time.time()\n    print('Time: ', e - s)","metadata":{"pycharm":{"name":"#%%\n"},"id":"bfu88nZWLWwO","execution":{"iopub.status.busy":"2023-05-22T20:25:17.983847Z","iopub.execute_input":"2023-05-22T20:25:17.984566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.run.summary['best_test_accuracy'] = best_acc \nwandb.run.summary['best_test_accuracy_compressed'] = best_acc_compressed\nwandb.finish()","metadata":{},"execution_count":null,"outputs":[]}]}